{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70fc407c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bac5a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation:\n",
    "    def __init__(self, type):\n",
    "        self.type = type\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        if self.type == \"relu\":\n",
    "            self.output = np.maximum(0, inputs)\n",
    "        elif self.type == \"sigmoid\":\n",
    "        # Clipping inputs to avoid overflow\n",
    "            clipped_inputs = np.clip(inputs, -709, 709)  # np.exp(709) is close to the limit\n",
    "            self.output = 1 / (1 + np.exp(-clipped_inputs))\n",
    "        elif self.type == \"linear\":\n",
    "            self.output = inputs\n",
    "        elif self.type == \"tanh\":\n",
    "            self.output = np.tanh(inputs)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid activation function type: {self.type}\")\n",
    "        return self.output\n",
    "\n",
    "    def derivative(self):\n",
    "        if self.type == \"relu\":\n",
    "            return np.where(self.inputs > 0, 1, 0)\n",
    "        elif self.type == \"sigmoid\":\n",
    "            return self.output * (1 - self.output)\n",
    "        elif self.type == \"linear\":\n",
    "            return np.ones_like(self.inputs)\n",
    "        elif self.type == \"tanh\":\n",
    "            return 1 - np.power(self.output, 2)\n",
    "        else:\n",
    "            raise ValueError(f\"No derivative implemented for activation function type: {self.type}\")\n",
    "\n",
    "class Parameters:\n",
    "    def __init__(self, input_size, num_neurons):\n",
    "        self.weights = np.random.randn(input_size, num_neurons) * 0.1\n",
    "        self.bias = np.random.randn(1, num_neurons) * 0.1\n",
    "\n",
    "    def get_weights(self):\n",
    "        return self.weights\n",
    "\n",
    "    def get_bias(self):\n",
    "        return self.bias\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, input_size):\n",
    "        self.params = Parameters(input_size, 1)  # Each neuron has one output\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs, self.params.get_weights()) + self.params.get_bias()\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, d_output):\n",
    "        # Reshape d_output if necessary to ensure it's 2D (batch_size, 1)\n",
    "        if d_output.ndim == 1:\n",
    "            d_output = d_output[:, np.newaxis]\n",
    "\n",
    "        self.d_weights = np.dot(self.inputs.T, d_output)\n",
    "        self.d_bias = np.sum(d_output, axis=0, keepdims=True)\n",
    "        self.d_inputs = np.dot(d_output, self.params.get_weights().T)\n",
    "\n",
    "        return self.d_inputs\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, input_size, num_neurons, activation_type):\n",
    "        self.neurons = [Neuron(input_size) for _ in range(num_neurons)]\n",
    "        self.activation_fn = Activation(activation_type)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        neuron_outputs = np.hstack([neuron.forward(inputs) for neuron in self.neurons])\n",
    "        return self.activation_fn.forward(neuron_outputs)\n",
    "\n",
    "    def backward(self, d_output):\n",
    "        d_output_activation = self.activation_fn.derivative() * d_output\n",
    "        d_inputs = np.zeros_like(self.inputs, dtype=np.float64)\n",
    "        for i, neuron in enumerate(self.neurons):\n",
    "            neuron_d_inputs = neuron.backward(d_output_activation[:, i:i+1])\n",
    "            d_inputs += neuron_d_inputs\n",
    "        return d_inputs\n",
    "\n",
    "class LossFunction:\n",
    "    @staticmethod\n",
    "    def mse(predicted, actual):\n",
    "        return np.mean((predicted - actual) ** 2)\n",
    "\n",
    "    @staticmethod\n",
    "    def mse_derivative(predicted, actual):\n",
    "        return 2 * (predicted - actual) / actual.size\n",
    "    \n",
    "class Dropout:\n",
    "    def __init__(self, rate):\n",
    "        self.rate = rate\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, inputs, training=True):\n",
    "        if training:\n",
    "            self.mask = np.random.binomial(1, 1 - self.rate, size=inputs.shape) / (1 - self.rate)\n",
    "            self.output = inputs * self.mask\n",
    "        else:\n",
    "            self.output = inputs\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, d_output):\n",
    "        # Only propagate gradients where the mask is not zero\n",
    "        self.d_inputs = d_output * self.mask\n",
    "        return self.d_inputs\n",
    "\n",
    "class Regularization:\n",
    "    @staticmethod\n",
    "    def l2(weights, lambda_val):\n",
    "        return (lambda_val / 2) * np.sum(weights ** 2)\n",
    "\n",
    "    @staticmethod\n",
    "    def l2_derivative(weights, lambda_val):\n",
    "        return lambda_val * weights\n",
    "    \n",
    "class BackPropagation:\n",
    "    @staticmethod\n",
    "    def update_parameters(layers, learning_rate, lambda_val=0.01):\n",
    "        for layer in layers:\n",
    "            if hasattr(layer, 'neurons'):\n",
    "                for neuron in layer.neurons:\n",
    "                    neuron.params.weights -= learning_rate * (neuron.d_weights + Regularization.l2_derivative(neuron.params.weights, lambda_val))\n",
    "                    neuron.params.bias -= learning_rate * neuron.d_bias\n",
    "                    \n",
    "                    \n",
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "\n",
    "    def add_layer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        for layer in self.layers:\n",
    "            inputs = layer.forward(inputs)\n",
    "        return inputs\n",
    "\n",
    "    def backward(self, loss_gradient):\n",
    "        for layer in reversed(self.layers):\n",
    "            loss_gradient = layer.backward(loss_gradient)\n",
    "            \n",
    "    def train(self, inputs, targets, epochs, learning_rate, lambda_val=0.01):\n",
    "        for epoch in range(epochs):\n",
    "            outputs = self.forward(inputs)\n",
    "            loss = LossFunction.mse(outputs, targets)\n",
    "            # Add regularization loss\n",
    "            reg_loss = 0\n",
    "            for layer in self.layers:\n",
    "                if hasattr(layer, 'neurons'):\n",
    "                    for neuron in layer.neurons:\n",
    "                        reg_loss += Regularization.l2(neuron.params.weights, lambda_val)\n",
    "            loss += reg_loss\n",
    "            print(f\"Epoch {epoch+1}, Loss: {loss}\")\n",
    "            loss_gradient = LossFunction.mse_derivative(outputs, targets)\n",
    "            self.backward(loss_gradient)\n",
    "            BackPropagation.update_parameters(self.layers, learning_rate, lambda_val)\n",
    "\n",
    "            \n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        predictions = self.predict(X)\n",
    "        # Assuming binary classification or single-label multiclass classification\n",
    "        predictions = np.round(predictions)  # Adjust as needed for your case\n",
    "        accuracy = np.mean(predictions == y)\n",
    "        return accuracy\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        return self.forward(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd6cc7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the Neural Network...\n",
      "Epoch 1, Loss: 0.7573292653346618\n",
      "Epoch 2, Loss: 0.6544925386363704\n",
      "Epoch 3, Loss: 0.5526002771078687\n",
      "Epoch 4, Loss: 0.4794461451376403\n",
      "Epoch 5, Loss: 0.43104675538796366\n",
      "Epoch 6, Loss: 0.4024135194631089\n",
      "Epoch 7, Loss: 0.38538172447442026\n",
      "Epoch 8, Loss: 0.3783976535199654\n",
      "Epoch 9, Loss: 0.3761530829882546\n",
      "Epoch 10, Loss: 0.3723172860180847\n",
      "Evaluating on Validation Data...\n",
      "Validation Accuracy: 0.887\n",
      "Evaluating on Test Data...\n",
      "Test Accuracy: 0.885\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = pd.read_csv(\"train_x.csv\")\n",
    "labels = pd.read_csv(\"train_label.csv\")\n",
    "\n",
    "# Analyzing the shapes of the dataset and labels\n",
    "dataset_shape = dataset.shape\n",
    "labels_shape = labels.shape\n",
    "\n",
    "dataset_shape, labels_shape\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the dataset into training, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(dataset, labels, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Checking the shapes of the splits\n",
    "(X_train.shape, y_train.shape), (X_val.shape, y_val.shape), (X_test.shape, y_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "# Convert the pandas dataframes to numpy arrays for processing with the neural network\n",
    "X_train_np = X_train.to_numpy()\n",
    "y_train_np = y_train.to_numpy()\n",
    "X_val_np = X_val.to_numpy()\n",
    "y_val_np = y_val.to_numpy()\n",
    "X_test_np = X_test.to_numpy()\n",
    "y_test_np = y_test.to_numpy()\n",
    "# Create the neural network\n",
    "nn = NeuralNetwork()\n",
    "\n",
    "input_size = 784  \n",
    "output_size = 10  \n",
    "\n",
    "nn.add_layer(Layer(input_size=input_size, num_neurons=64, activation_type='relu'))\n",
    "nn.add_layer(Dropout(rate=0.1))  # Dropout layer with a 50% dropout rate\n",
    "nn.add_layer(Layer(input_size=64, num_neurons=output_size, activation_type='sigmoid'))\n",
    "\n",
    "\n",
    "# Training parameters\n",
    "epochs = 10\n",
    "learning_rate = 0.1\n",
    "lambda_val = 0.001\n",
    "# Training the neural network\n",
    "print(\"Training the Neural Network...\")\n",
    "nn.train(X_train_np, y_train_np, epochs, learning_rate, lambda_val)\n",
    "\n",
    "# Evaluating the neural network\n",
    "print(\"Evaluating on Validation Data...\")\n",
    "val_accuracy = nn.evaluate(X_val_np, y_val_np)\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "print(\"Evaluating on Test Data...\")\n",
    "test_accuracy = nn.evaluate(X_test_np, y_test_np)\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b651f56e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
