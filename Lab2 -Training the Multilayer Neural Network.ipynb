{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e04df284",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fefe4849",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2956a3",
   "metadata": {},
   "source": [
    "# Neuron Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ce53b8",
   "metadata": {},
   "source": [
    "Activation Computation and output based on the provided aggregate signal. Each neuron has one output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87ed6600",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, input_size):\n",
    "        self.params = Parameters(input_size, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0961f13",
   "metadata": {},
   "source": [
    "# Parameter Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8a8de6",
   "metadata": {},
   "source": [
    "Manages weights, bias, learning rate, regularization, and dropout or any other parameters for a layer during neural network training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc687bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameters:\n",
    "    def __init__(self, input_size, num_neurons):\n",
    "        self.weights = np.random.randn(input_size, num_neurons) * 0.1\n",
    "        self.bias = np.random.randn(1, num_neurons) * 0.1\n",
    "\n",
    "    def get_weights(self):\n",
    "        return self.weights\n",
    "\n",
    "    def get_bias(self):\n",
    "        return self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dc509e",
   "metadata": {},
   "source": [
    "# Layer Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83333ec",
   "metadata": {},
   "source": [
    "Represents a layer in the neural network, applying forward propagation and managing its parameters and activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3e56f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, input_size, num_neurons, activation_type):\n",
    "        self.neurons = [Neuron(input_size) for _ in range(num_neurons)]\n",
    "        self.activation_fn = Activation(activation_type)\n",
    "        self.input_size = input_size\n",
    "        self.num_neurons = num_neurons\n",
    "        self.activation_type = activation_type\n",
    "        self.inputs = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0f655b",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd92fe1",
   "metadata": {},
   "source": [
    "Defines various loss functions (MSE, RMSE, Cross-Entropy) for evaluating model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be715490",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossFunction:\n",
    "    @staticmethod\n",
    "    def mse(predicted, actual):\n",
    "        return np.mean((predicted - actual) ** 2)\n",
    "\n",
    "    @staticmethod\n",
    "    def mse_derivative(predicted, actual):\n",
    "        return 2 * (predicted - actual) / actual.size\n",
    "    \n",
    "    @staticmethod\n",
    "    def binary_cross_entropy(predicted, actual):\n",
    "        epsilon = 1e-15\n",
    "        predicted = np.clip(predicted, epsilon, 1 - epsilon)\n",
    "        return -np.mean(actual * np.log(predicted) + (1 - actual) * np.log(1 - predicted))\n",
    "\n",
    "    @staticmethod\n",
    "    def binary_cross_entropy_derivative(predicted, actual):\n",
    "        epsilon = 1e-15\n",
    "        predicted = np.clip(predicted, epsilon, 1 - epsilon)\n",
    "        return (predicted - actual) / (predicted * (1 - predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e432d4f6",
   "metadata": {},
   "source": [
    "# Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90685671",
   "metadata": {},
   "source": [
    "Implements various activation functions (ReLU, Sigmoid, Tanh, Softmax) and computes the activation or its derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7400ab2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation:\n",
    "    def __init__(self, type):\n",
    "        self.type = type\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        if self.type == \"relu\":\n",
    "            self.output = np.maximum(0, inputs)\n",
    "        elif self.type == \"sigmoid\":\n",
    "        # Clipping inputs to avoid overflow\n",
    "            clipped_inputs = np.clip(inputs, -709, 709)  # np.exp(709) is close to the limit\n",
    "            self.output = 1 / (1 + np.exp(-clipped_inputs))\n",
    "        elif self.type == \"linear\":\n",
    "            self.output = inputs\n",
    "        elif self.type == \"tanh\":\n",
    "            self.output = np.tanh(inputs)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid activation function type: {self.type}\")\n",
    "        return self.output\n",
    "\n",
    "    def derivative(self):\n",
    "        if self.type == \"relu\":\n",
    "            return np.where(self.inputs > 0, 1, 0)\n",
    "        elif self.type == \"sigmoid\":\n",
    "            return self.output * (1 - self.output)\n",
    "        elif self.type == \"linear\":\n",
    "            return np.ones_like(self.inputs)\n",
    "        elif self.type == \"tanh\":\n",
    "            return 1 - np.power(self.output, 2)\n",
    "        else:\n",
    "            raise ValueError(f\"No derivative implemented for activation function type: {self.type}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a676ecc2",
   "metadata": {},
   "source": [
    "# Dropout & Regularization "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbf2739",
   "metadata": {},
   "source": [
    "Implements various activation functions (ReLU, Sigmoid, Tanh, Softmax) and computes the activation or its derivative.\n",
    "Computes regularization term for weights based on the specified regularization type (\"l1\" or \"l2\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46646898",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    def __init__(self, rate):\n",
    "        self.rate = rate\n",
    "        self.mask = None\n",
    "\n",
    "class Regularization:\n",
    "    @staticmethod\n",
    "    def l1(weights, lambda_val):\n",
    "        return lambda_val * np.sum(np.abs(weights))\n",
    "\n",
    "    @staticmethod\n",
    "    def l1_derivative(weights, lambda_val):\n",
    "        return lambda_val * np.sign(weights)\n",
    "\n",
    "    @staticmethod\n",
    "    def l2(weights, lambda_val):\n",
    "        return (lambda_val / 2) * np.sum(np.square(weights))\n",
    "\n",
    "    @staticmethod\n",
    "    def l2_derivative(weights, lambda_val):\n",
    "        return lambda_val * weights\n",
    "\n",
    "    @staticmethod\n",
    "    def compute(reg_type, weights, lambda_val):\n",
    "        if reg_type == \"l1\":\n",
    "            return Regularization.l1(weights, lambda_val)\n",
    "        elif reg_type == \"l2\":\n",
    "            return Regularization.l2(weights, lambda_val)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid regularization type. Expected 'l1' or 'l2'.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def derivative(reg_type, weights, lambda_val):\n",
    "        if reg_type == \"l1\":\n",
    "            return Regularization.l1_derivative(weights, lambda_val)\n",
    "        elif reg_type == \"l2\":\n",
    "            return Regularization.l2_derivative(weights, lambda_val)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid regularization type. Expected 'l1' or 'l2'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00845fe1",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade4f8b6",
   "metadata": {},
   "source": [
    "Updates weights and biases using gradient descent during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e67b9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescent:\n",
    "    @staticmethod\n",
    "    def update_parameters(layers, learning_rate, lambda_val=0.01):\n",
    "        for layer in layers:\n",
    "            if hasattr(layer, 'neurons'):\n",
    "                for neuron in layer.neurons:\n",
    "                    neuron.params.weights -= learning_rate * neuron.d_weights\n",
    "                    neuron.params.bias -= learning_rate * neuron.d_bias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b366d87a",
   "metadata": {},
   "source": [
    "# Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4af9ead",
   "metadata": {},
   "source": [
    "Applies forward propagation for the entire neural network, computing outputs for each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10ff36e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForwardPropagation:\n",
    "    @staticmethod\n",
    "    def apply_dropout(layer, inputs, training=True):\n",
    "        if not isinstance(layer, Dropout):\n",
    "            return inputs  # If the layer is not a Dropout layer, just return the inputs\n",
    "\n",
    "        if training:\n",
    "            # Create a mask and apply it\n",
    "            mask = np.random.binomial(1, 1 - layer.rate, size=inputs.shape) / (1 - layer.rate)\n",
    "            return inputs * mask\n",
    "        else:\n",
    "            return inputs  # No dropout during evaluation\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward_layer(layer, inputs):\n",
    "        # Save the inputs to the layer\n",
    "        layer.inputs = inputs\n",
    "        neuron_outputs = np.hstack([np.dot(inputs, neuron.params.get_weights()) + neuron.params.get_bias() for neuron in layer.neurons])\n",
    "        return layer.activation_fn.forward(neuron_outputs)\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(network, inputs):\n",
    "        for layer in network.layers:\n",
    "            inputs = ForwardPropagation.forward_layer(layer, inputs)\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540d49b3",
   "metadata": {},
   "source": [
    "# Back Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a290e63",
   "metadata": {},
   "source": [
    "Performs backward propagation to compute gradients and update weights based on the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d28cafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackwardPropagation:\n",
    "    @staticmethod\n",
    "    def backward_layer(layer, d_output):\n",
    "        d_output_activation = layer.activation_fn.derivative() * d_output\n",
    "        d_inputs = np.zeros((d_output_activation.shape[0], layer.input_size))\n",
    "        for i, neuron in enumerate(layer.neurons):\n",
    "            d_output_neuron = d_output_activation[:, i:i+1]\n",
    "            neuron.d_weights = np.dot(layer.inputs.T, d_output_neuron)\n",
    "            neuron.d_bias = np.sum(d_output_neuron, axis=0, keepdims=True)\n",
    "            d_inputs += np.dot(d_output_neuron, neuron.params.get_weights().T)\n",
    "        layer.inputs = d_inputs  # Update layer inputs for next layer backward pass\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_gradients(network, loss_gradient):\n",
    "        for layer in reversed(network.layers):\n",
    "            BackwardPropagation.backward_layer(layer, loss_gradient)\n",
    "            loss_gradient = layer.inputs  # Use updated layer inputs as next gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf43aa2",
   "metadata": {},
   "source": [
    "# Input Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dd372a",
   "metadata": {},
   "source": [
    "Normalizes input data by calculating mean and standard deviation for better convergence during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ac90b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputNormalization:\n",
    "    def __init__(self, method='zscore'):\n",
    "        self.method = method\n",
    "        self.min = None\n",
    "        self.max = None\n",
    "        self.mean = None\n",
    "        self.std = None\n",
    "\n",
    "    def fit(self, data):\n",
    "        if self.method == 'minmax':\n",
    "            self.min = np.min(data, axis=0)\n",
    "            self.max = np.max(data, axis=0)\n",
    "        elif self.method == 'zscore':\n",
    "            self.mean = np.mean(data, axis=0)\n",
    "            self.std = np.std(data, axis=0)\n",
    "            self.std[self.std == 0] = 1  # Prevent division by zero for features with no variation\n",
    "        elif self.method == 'max':\n",
    "            self.max = np.max(np.abs(data), axis=0)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown normalization method: {self.method}\")\n",
    "\n",
    "    def transform(self, data):\n",
    "        if self.method == 'minmax':\n",
    "            return (data - self.min) / (self.max - self.min)\n",
    "        elif self.method == 'zscore':\n",
    "            return (data - self.mean) / self.std\n",
    "        elif self.method == 'max':\n",
    "            return data / self.max\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown normalization method: {self.method}\")\n",
    "\n",
    "    def fit_transform(self, data):\n",
    "        self.fit(data)\n",
    "        return self.transform(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8da1b9",
   "metadata": {},
   "source": [
    "# Mini Batch Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f4413b",
   "metadata": {},
   "source": [
    "Creates mini-batches from the training data to facilitate training with smaller subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1ea2dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniBatchGenerator:\n",
    "    def __init__(self, X, y, batch_size=32):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.n_samples = X.shape[0]\n",
    "        self.indices = np.arange(self.n_samples)\n",
    "        np.random.shuffle(self.indices)\n",
    "\n",
    "    def get_batches(self):\n",
    "        for start_idx in range(0, self.n_samples, self.batch_size):\n",
    "            end_idx = min(start_idx + self.batch_size, self.n_samples)\n",
    "            batch_indices = self.indices[start_idx:end_idx]\n",
    "            yield self.X[batch_indices], self.y[batch_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0eca3c",
   "metadata": {},
   "source": [
    "# Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2e743a",
   "metadata": {},
   "source": [
    "Represents the entire neural network model, managing layers and their configurations for training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c5c1335",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "\n",
    "    def add_layer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Utilize the ForwardPropagation class for the forward pass\n",
    "        return ForwardPropagation.forward(self, inputs)\n",
    "\n",
    "    def backward(self, loss_gradient):\n",
    "        # Utilize the BackwardPropagation class for computing gradients\n",
    "        BackwardPropagation.compute_gradients(self, loss_gradient)\n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        predictions = self.predict(X)\n",
    "        predictions = np.round(predictions)  # Adjust as needed for your case\n",
    "        accuracy = np.mean(predictions == y)\n",
    "        return accuracy\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        return self.forward(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426441a1",
   "metadata": {},
   "source": [
    "# Training Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb6d1a7",
   "metadata": {},
   "source": [
    "Manages the training process, including forward and backward propagation, updating parameters, and evaluating performance over multiple epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7141c8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkTrainer:\n",
    "    def __init__(self, neural_network, learning_rate, lambda_val, batch_size):\n",
    "        self.neural_network = neural_network\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lambda_val = lambda_val\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def train(self, X_train, y_train, epochs):\n",
    "        # Optionally, you could implement mini-batch training by integrating the MiniBatchGenerator here\n",
    "        for epoch in range(epochs):\n",
    "            # If using mini-batches, this loop would iterate over batches instead of the entire dataset at once\n",
    "            outputs = self.neural_network.forward(X_train)\n",
    "            loss = LossFunction.binary_cross_entropy(outputs, y_train) + self.regularization_loss()\n",
    "\n",
    "            print(f\"Epoch {epoch+1}, Loss: {loss}\")\n",
    "            loss_gradient = LossFunction.mse_derivative(outputs, y_train)\n",
    "\n",
    "            self.neural_network.backward(loss_gradient)\n",
    "            GradientDescent.update_parameters(self.neural_network.layers, self.learning_rate, self.lambda_val)\n",
    "\n",
    "    def regularization_loss(self):\n",
    "        # Calculate and return the regularization loss\n",
    "        reg_loss = 0\n",
    "        reg_type = \"l2\"  # or \"l1\" depending on the chosen regularization\n",
    "        for layer in self.neural_network.layers:\n",
    "            for neuron in layer.neurons:\n",
    "                reg_loss += Regularization.compute(reg_type, neuron.params.weights, self.lambda_val)\n",
    "        return reg_loss\n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        # Utilize the NeuralNetwork's evaluate method\n",
    "        return self.neural_network.evaluate(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6282eb07",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e230add",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((999, 784), (999, 10))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"train_x.csv\")\n",
    "labels = pd.read_csv(\"train_label.csv\")\n",
    "\n",
    "# Analyzing the shapes of the dataset and labels\n",
    "dataset_shape = dataset.shape\n",
    "labels_shape = labels.shape\n",
    "\n",
    "dataset_shape, labels_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018aa21d",
   "metadata": {},
   "source": [
    "# Split Data into Train Test and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97746bb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((799, 784), (799, 10)), ((100, 784), (100, 10)), ((100, 784), (100, 10)))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting the dataset into training, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(dataset, labels, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Checking the shapes of the splits\n",
    "(X_train.shape, y_train.shape), (X_val.shape, y_val.shape), (X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9321103",
   "metadata": {},
   "source": [
    "# Creating Numpy Arrays for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a4a2c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_normalizer = InputNormalization(method='zscore')\n",
    "# Convert the pandas dataframes to numpy arrays for processing with the neural network\n",
    "X_train_np = input_normalizer.fit_transform(X_train.to_numpy())\n",
    "X_val_np = input_normalizer.transform(X_val.to_numpy())\n",
    "X_test_np = input_normalizer.transform(X_test.to_numpy())\n",
    "\n",
    "y_train_np = y_train.to_numpy()\n",
    "y_val_np = y_val.to_numpy()\n",
    "y_test_np = y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411ad3ba",
   "metadata": {},
   "source": [
    "# Setting the Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e9f1c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784  \n",
    "output_size = 10  \n",
    "epochs = 100\n",
    "learning_rate = 0.1\n",
    "lambda_val = 0.001\n",
    "batch_size = 32  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b010dc5",
   "metadata": {},
   "source": [
    "# Setting up the Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1cf0fab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork()\n",
    "nn.add_layer(Layer(input_size=input_size, num_neurons=10, activation_type='relu'))\n",
    "nn.add_layer(Layer(input_size=10, num_neurons=8, activation_type='relu'))\n",
    "nn.add_layer(Layer(input_size=8, num_neurons=8, activation_type='relu'))  # Second layer with 8 neurons and ReLU\n",
    "nn.add_layer(Layer(input_size=8, num_neurons=4, activation_type='relu'))\n",
    "nn.add_layer(Layer(input_size=4, num_neurons=1, activation_type='sigmoid')) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ee55d3",
   "metadata": {},
   "source": [
    "# Instantiate the training Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "28e2668e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = NetworkTrainer(nn, learning_rate, lambda_val, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed91fb8a",
   "metadata": {},
   "source": [
    "# Training the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64bc8523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the Neural Network...\n",
      "Epoch 1, Loss: 0.6951928046850713\n",
      "Epoch 2, Loss: 0.6944568896161956\n",
      "Epoch 3, Loss: 0.6937230251629322\n",
      "Epoch 4, Loss: 0.6929912062037775\n",
      "Epoch 5, Loss: 0.6922614273682923\n",
      "Epoch 6, Loss: 0.6915336842470325\n",
      "Epoch 7, Loss: 0.6908079701593525\n",
      "Epoch 8, Loss: 0.6900842795394712\n",
      "Epoch 9, Loss: 0.6893626078951945\n",
      "Epoch 10, Loss: 0.688642948618038\n",
      "Epoch 11, Loss: 0.6879252963168282\n",
      "Epoch 12, Loss: 0.6872096449556718\n",
      "Epoch 13, Loss: 0.6864959897320162\n",
      "Epoch 14, Loss: 0.6857843270418112\n",
      "Epoch 15, Loss: 0.6850746511049167\n",
      "Epoch 16, Loss: 0.6843669557919659\n",
      "Epoch 17, Loss: 0.6836612340888397\n",
      "Epoch 18, Loss: 0.6829574805613156\n",
      "Epoch 19, Loss: 0.6822556902622893\n",
      "Epoch 20, Loss: 0.6815558581138484\n",
      "Epoch 21, Loss: 0.6808579783029808\n",
      "Epoch 22, Loss: 0.6801620478091187\n",
      "Epoch 23, Loss: 0.6794680598678878\n",
      "Epoch 24, Loss: 0.6787760080208972\n",
      "Epoch 25, Loss: 0.6780859233591173\n",
      "Epoch 26, Loss: 0.6773977639710044\n",
      "Epoch 27, Loss: 0.6767115251762853\n",
      "Epoch 28, Loss: 0.6760272030581622\n",
      "Epoch 29, Loss: 0.6753447915639039\n",
      "Epoch 30, Loss: 0.6746642851797331\n",
      "Epoch 31, Loss: 0.6739857150496018\n",
      "Epoch 32, Loss: 0.6733090487558551\n",
      "Epoch 33, Loss: 0.6726342951137877\n",
      "Epoch 34, Loss: 0.6719614236585585\n",
      "Epoch 35, Loss: 0.6712904282910405\n",
      "Epoch 36, Loss: 0.6706213037461942\n",
      "Epoch 37, Loss: 0.6699540514816831\n",
      "Epoch 38, Loss: 0.6692887021679337\n",
      "Epoch 39, Loss: 0.6686252326490494\n",
      "Epoch 40, Loss: 0.6679636123724433\n",
      "Epoch 41, Loss: 0.6673038365251261\n",
      "Epoch 42, Loss: 0.6666458988193358\n",
      "Epoch 43, Loss: 0.6659898028815928\n",
      "Epoch 44, Loss: 0.665335560213219\n",
      "Epoch 45, Loss: 0.6646831394964529\n",
      "Epoch 46, Loss: 0.6640325359276902\n",
      "Epoch 47, Loss: 0.663383755156466\n",
      "Epoch 48, Loss: 0.6627368243142786\n",
      "Epoch 49, Loss: 0.6620917478165385\n",
      "Epoch 50, Loss: 0.6614484671293761\n",
      "Epoch 51, Loss: 0.6608069924626121\n",
      "Epoch 52, Loss: 0.6601673378888713\n",
      "Epoch 53, Loss: 0.6595295148133594\n",
      "Epoch 54, Loss: 0.658893609419827\n",
      "Epoch 55, Loss: 0.6582595227808505\n",
      "Epoch 56, Loss: 0.6576272610621451\n",
      "Epoch 57, Loss: 0.656996810317916\n",
      "Epoch 58, Loss: 0.6563681147137644\n",
      "Epoch 59, Loss: 0.655741236643759\n",
      "Epoch 60, Loss: 0.6551161256654886\n",
      "Epoch 61, Loss: 0.6544927626828071\n",
      "Epoch 62, Loss: 0.653871265005822\n",
      "Epoch 63, Loss: 0.6532515201679948\n",
      "Epoch 64, Loss: 0.6526335135749041\n",
      "Epoch 65, Loss: 0.6520173495892911\n",
      "Epoch 66, Loss: 0.6514029628758171\n",
      "Epoch 67, Loss: 0.6507904622036299\n",
      "Epoch 68, Loss: 0.6501797926481842\n",
      "Epoch 69, Loss: 0.6495709307069417\n",
      "Epoch 70, Loss: 0.6489638966546668\n",
      "Epoch 71, Loss: 0.648358728267021\n",
      "Epoch 72, Loss: 0.6477554440647949\n",
      "Epoch 73, Loss: 0.6471539481191885\n",
      "Epoch 74, Loss: 0.646554362729938\n",
      "Epoch 75, Loss: 0.6459567032143038\n",
      "Epoch 76, Loss: 0.6453609261890738\n",
      "Epoch 77, Loss: 0.6447669474838024\n",
      "Epoch 78, Loss: 0.6441747644131925\n",
      "Epoch 79, Loss: 0.6435843672267278\n",
      "Epoch 80, Loss: 0.6429957388900805\n",
      "Epoch 81, Loss: 0.6424088034660388\n",
      "Epoch 82, Loss: 0.641823650013305\n",
      "Epoch 83, Loss: 0.6412402246075976\n",
      "Epoch 84, Loss: 0.6406585326882802\n",
      "Epoch 85, Loss: 0.6400786456547524\n",
      "Epoch 86, Loss: 0.6395004413528418\n",
      "Epoch 87, Loss: 0.6389239935199446\n",
      "Epoch 88, Loss: 0.638349338126938\n",
      "Epoch 89, Loss: 0.6377763931277097\n",
      "Epoch 90, Loss: 0.6372050743809204\n",
      "Epoch 91, Loss: 0.6366353780118221\n",
      "Epoch 92, Loss: 0.6360673432547047\n",
      "Epoch 93, Loss: 0.6355009727030471\n",
      "Epoch 94, Loss: 0.6349362603458275\n",
      "Epoch 95, Loss: 0.6343731632704831\n",
      "Epoch 96, Loss: 0.6338116382633916\n",
      "Epoch 97, Loss: 0.6332517230519987\n",
      "Epoch 98, Loss: 0.6326934430314923\n",
      "Epoch 99, Loss: 0.6321367676805608\n",
      "Epoch 100, Loss: 0.6315817163819473\n"
     ]
    }
   ],
   "source": [
    "# Training the neural network using the trainer\n",
    "print(\"Training the Neural Network...\")\n",
    "trainer.train(X_train_np, y_train_np, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a38fa9",
   "metadata": {},
   "source": [
    "# Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "efd79902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on Validation Data...\n",
      "Validation Accuracy: 0.9\n",
      "Evaluating on Test Data...\n",
      "Test Accuracy: 0.9\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the neural network\n",
    "print(\"Evaluating on Validation Data...\")\n",
    "val_accuracy = nn.evaluate(X_val_np, y_val_np)\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "print(\"Evaluating on Test Data...\")\n",
    "test_accuracy = nn.evaluate(X_test_np, y_test_np)\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f01ef3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
