{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed8d747e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28646c16",
   "metadata": {},
   "source": [
    "# Activation Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3c41301",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation:\n",
    "    @staticmethod\n",
    "    def tanh(x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(x):\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum(axis=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def derivative_tanh(x):\n",
    "        return 1 - np.power(np.tanh(x), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7093e668",
   "metadata": {},
   "source": [
    "# Parameters Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3110e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameters:\n",
    "    def __init__(self, n_x, n_h, n_y):\n",
    "        self.w1 = np.random.randn(n_h, n_x) * 0.01\n",
    "        self.b1 = np.zeros((n_h, 1))\n",
    "        self.w2 = np.random.randn(n_y, n_h) * 0.01\n",
    "        self.b2 = np.zeros((n_y, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f69a8e",
   "metadata": {},
   "source": [
    "# Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7bddc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForwardProp:\n",
    "    @staticmethod\n",
    "    def forward(x, parameters):\n",
    "        z1 = np.dot(parameters.w1, x) + parameters.b1\n",
    "        a1 = Activation.tanh(z1)\n",
    "        z2 = np.dot(parameters.w2, a1) + parameters.b2\n",
    "        a2 = Activation.softmax(z2)\n",
    "\n",
    "        cache = {\"z1\": z1, \"a1\": a1, \"z2\": z2, \"a2\": a2}\n",
    "        return cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1630cdcc",
   "metadata": {},
   "source": [
    "# Loss Function Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d42b646a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossFunction:\n",
    "    @staticmethod\n",
    "    def compute_cost(a2, y):\n",
    "        m = y.shape[1]\n",
    "        cost = -(1 / m) * np.sum(y * np.log(a2))\n",
    "        return cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30477e21",
   "metadata": {},
   "source": [
    "# Back Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9946225c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackProp:\n",
    "    @staticmethod\n",
    "    def backward(x, y, parameters, cache):\n",
    "        m = x.shape[1]\n",
    "        dz2 = cache['a2'] - y\n",
    "        dw2 = (1 / m) * np.dot(dz2, cache['a1'].T)\n",
    "        db2 = (1 / m) * np.sum(dz2, axis=1, keepdims=True)\n",
    "        dz1 = (1 / m) * np.dot(parameters.w2.T, dz2) * Activation.derivative_tanh(cache['a1'])\n",
    "        dw1 = (1 / m) * np.dot(dz1, x.T)\n",
    "        db1 = (1 / m) * np.sum(dz1, axis=1, keepdims=True)\n",
    "\n",
    "        gradients = {\"dw1\": dw1, \"db1\": db1, \"dw2\": dw2, \"db2\": db2}\n",
    "        return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d05fdc3",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4366b072",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradDescent:\n",
    "    @staticmethod\n",
    "    def update(parameters, gradients, learning_rate):\n",
    "        parameters.w1 -= learning_rate * gradients['dw1']\n",
    "        parameters.b1 -= learning_rate * gradients['db1']\n",
    "        parameters.w2 -= learning_rate * gradients['dw2']\n",
    "        parameters.b2 -= learning_rate * gradients['db2']\n",
    "        return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46e97f9",
   "metadata": {},
   "source": [
    "# Training Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6630cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Training:\n",
    "    def __init__(self, n_h, learning_rate, iterations):\n",
    "        self.n_h = n_h\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iterations = iterations\n",
    "\n",
    "    def train(self, x, y):\n",
    "        n_x, n_y = x.shape[0], y.shape[0]\n",
    "        parameters = Parameters(n_x, self.n_h, n_y)\n",
    "        cost_list = []\n",
    "\n",
    "        for i in range(self.iterations):\n",
    "            cache = ForwardProp.forward(x, parameters)\n",
    "            cost = LossFunction.compute_cost(cache['a2'], y)\n",
    "            gradients = BackProp.backward(x, y, parameters, cache)\n",
    "            parameters = GradDescent.update(parameters, gradients, self.learning_rate)\n",
    "\n",
    "            cost_list.append(cost)\n",
    "            if i % (self.iterations / 10) == 0:\n",
    "                print(\"Cost after\", i, \"iterations is:\", cost)\n",
    "\n",
    "        return parameters, cost_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8069f182",
   "metadata": {},
   "source": [
    "# Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08a38ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, n_h, learning_rate, iterations):\n",
    "        self.training = Training(n_h, learning_rate, iterations)\n",
    "\n",
    "    def fit(self, X_train, Y_train):\n",
    "        return self.training.train(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a057a88f",
   "metadata": {},
   "source": [
    "# Loading Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "849c411b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.loadtxt('train_X.csv', delimiter = ',').T\n",
    "Y_train = np.loadtxt('train_label.csv', delimiter = ',').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3952a83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after 0 iterations is: 2.4056438351896285\n",
      "Cost after 10 iterations is: 2.2953041782224792\n",
      "Cost after 20 iterations is: 2.195767668320458\n",
      "Cost after 30 iterations is: 2.1046961664419057\n",
      "Cost after 40 iterations is: 2.0206753250765153\n",
      "Cost after 50 iterations is: 1.9427901334494948\n",
      "Cost after 60 iterations is: 1.8703902153163086\n",
      "Cost after 70 iterations is: 1.8029667162184726\n",
      "Cost after 80 iterations is: 1.7400911359001228\n",
      "Cost after 90 iterations is: 1.6813861619228583\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the Model class\n",
    "neural_network_model = Model(n_h=n_h, learning_rate=learning_rate, iterations=iterations)\n",
    "\n",
    "# Train the model using the training data\n",
    "parameters, cost_list = neural_network_model.fit(X_train, Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d3abe4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
