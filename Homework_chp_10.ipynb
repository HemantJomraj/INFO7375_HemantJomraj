{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c36c06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the Neural Network...\n",
      "Epoch 1, Loss: 0.5906435743153546\n",
      "Epoch 2, Loss: 0.5745391818120322\n",
      "Epoch 3, Loss: 0.5637463283127535\n",
      "Epoch 4, Loss: 0.5529279587746171\n",
      "Epoch 5, Loss: 0.5411514950689724\n",
      "Epoch 6, Loss: 0.5287311708470934\n",
      "Epoch 7, Loss: 0.517508605619935\n",
      "Epoch 8, Loss: 0.5073821888861842\n",
      "Epoch 9, Loss: 0.4983736356737441\n",
      "Epoch 10, Loss: 0.48947842585612167\n",
      "Epoch 11, Loss: 0.4824712721361907\n",
      "Epoch 12, Loss: 0.47235479937940783\n",
      "Epoch 13, Loss: 0.46427635822070784\n",
      "Epoch 14, Loss: 0.45810844813568785\n",
      "Epoch 15, Loss: 0.45153322885712016\n",
      "Epoch 16, Loss: 0.44502659187418747\n",
      "Epoch 17, Loss: 0.4382080073924419\n",
      "Epoch 18, Loss: 0.43342202717574896\n",
      "Epoch 19, Loss: 0.42777992116934277\n",
      "Epoch 20, Loss: 0.42340063572218756\n",
      "Epoch 21, Loss: 0.41650189966210927\n",
      "Epoch 22, Loss: 0.4122749804282665\n",
      "Epoch 23, Loss: 0.409197142950604\n",
      "Epoch 24, Loss: 0.404082608914082\n",
      "Epoch 25, Loss: 0.4007703409979039\n",
      "Epoch 26, Loss: 0.39659871442668115\n",
      "Epoch 27, Loss: 0.3931550015902538\n",
      "Epoch 28, Loss: 0.3908067237041432\n",
      "Epoch 29, Loss: 0.3880021735728312\n",
      "Epoch 30, Loss: 0.38481898862666375\n",
      "Epoch 31, Loss: 0.3837153922123395\n",
      "Epoch 32, Loss: 0.3794149205823557\n",
      "Epoch 33, Loss: 0.37935094141259895\n",
      "Epoch 34, Loss: 0.37458520524229305\n",
      "Epoch 35, Loss: 0.37379573190812854\n",
      "Epoch 36, Loss: 0.3727454595012447\n",
      "Epoch 37, Loss: 0.37194770523542237\n",
      "Epoch 38, Loss: 0.37103637686600666\n",
      "Epoch 39, Loss: 0.3683772546898497\n",
      "Epoch 40, Loss: 0.36578425917441004\n",
      "Epoch 41, Loss: 0.36604437849343885\n",
      "Epoch 42, Loss: 0.36418848308096524\n",
      "Epoch 43, Loss: 0.36309865682016806\n",
      "Epoch 44, Loss: 0.36096276154229107\n",
      "Epoch 45, Loss: 0.36003895393547775\n",
      "Epoch 46, Loss: 0.3591715128056642\n",
      "Epoch 47, Loss: 0.3576632366101286\n",
      "Epoch 48, Loss: 0.3579886654013984\n",
      "Epoch 49, Loss: 0.3569541706060952\n",
      "Epoch 50, Loss: 0.35555848783686356\n",
      "Epoch 51, Loss: 0.3545056929802348\n",
      "Epoch 52, Loss: 0.35406187607957845\n",
      "Epoch 53, Loss: 0.3529173835536206\n",
      "Epoch 54, Loss: 0.3520447198316691\n",
      "Epoch 55, Loss: 0.35102132763946714\n",
      "Epoch 56, Loss: 0.3504102906466031\n",
      "Epoch 57, Loss: 0.3497673127195186\n",
      "Epoch 58, Loss: 0.3495790805307877\n",
      "Epoch 59, Loss: 0.34917681966882774\n",
      "Epoch 60, Loss: 0.3485509171034516\n",
      "Epoch 61, Loss: 0.3478035830990798\n",
      "Epoch 62, Loss: 0.347442800360691\n",
      "Epoch 63, Loss: 0.34709567972069566\n",
      "Epoch 64, Loss: 0.3454471125254407\n",
      "Epoch 65, Loss: 0.3455670414250553\n",
      "Epoch 66, Loss: 0.34616674443851836\n",
      "Epoch 67, Loss: 0.344468293677823\n",
      "Epoch 68, Loss: 0.34376468409458094\n",
      "Epoch 69, Loss: 0.3439716708938962\n",
      "Epoch 70, Loss: 0.34355870895840246\n",
      "Epoch 71, Loss: 0.3423859272959872\n",
      "Epoch 72, Loss: 0.34201385196685213\n",
      "Epoch 73, Loss: 0.3415667467556481\n",
      "Epoch 74, Loss: 0.3414628858060841\n",
      "Epoch 75, Loss: 0.34131736741060414\n",
      "Epoch 76, Loss: 0.33966473637922856\n",
      "Epoch 77, Loss: 0.3405414169432863\n",
      "Epoch 78, Loss: 0.3396915794831778\n",
      "Epoch 79, Loss: 0.3397499647212649\n",
      "Epoch 80, Loss: 0.33943808834516487\n",
      "Epoch 81, Loss: 0.3392507375790281\n",
      "Epoch 82, Loss: 0.338134260039279\n",
      "Epoch 83, Loss: 0.33829217810313794\n",
      "Epoch 84, Loss: 0.33806914678277267\n",
      "Epoch 85, Loss: 0.3376417323653733\n",
      "Epoch 86, Loss: 0.33799085720474614\n",
      "Epoch 87, Loss: 0.3360177942031918\n",
      "Epoch 88, Loss: 0.3359165137351849\n",
      "Epoch 89, Loss: 0.33657912512700494\n",
      "Epoch 90, Loss: 0.33524786500284354\n",
      "Epoch 91, Loss: 0.3356751193187396\n",
      "Epoch 92, Loss: 0.33525005828737675\n",
      "Epoch 93, Loss: 0.3357530064910635\n",
      "Epoch 94, Loss: 0.33513326636326446\n",
      "Epoch 95, Loss: 0.33398225295822204\n",
      "Epoch 96, Loss: 0.33331808307605193\n",
      "Epoch 97, Loss: 0.3337100011794862\n",
      "Epoch 98, Loss: 0.33359951751830536\n",
      "Epoch 99, Loss: 0.33318333245786014\n",
      "Epoch 100, Loss: 0.3335180795229678\n",
      "Evaluating on Validation Data...\n",
      "Validation Accuracy: 0.894\n",
      "Evaluating on Test Data...\n",
      "Test Accuracy: 0.899\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class Activation:\n",
    "    def __init__(self, type):\n",
    "        self.type = type\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        if self.type == \"relu\":\n",
    "            self.output = np.maximum(0, inputs)\n",
    "        elif self.type == \"sigmoid\":\n",
    "        # Clipping inputs to avoid overflow\n",
    "            clipped_inputs = np.clip(inputs, -709, 709)  # np.exp(709) is close to the limit\n",
    "            self.output = 1 / (1 + np.exp(-clipped_inputs))\n",
    "        elif self.type == \"linear\":\n",
    "            self.output = inputs\n",
    "        elif self.type == \"tanh\":\n",
    "            self.output = np.tanh(inputs)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid activation function type: {self.type}\")\n",
    "        return self.output\n",
    "\n",
    "    def derivative(self):\n",
    "        if self.type == \"relu\":\n",
    "            return np.where(self.inputs > 0, 1, 0)\n",
    "        elif self.type == \"sigmoid\":\n",
    "            return self.output * (1 - self.output)\n",
    "        elif self.type == \"linear\":\n",
    "            return np.ones_like(self.inputs)\n",
    "        elif self.type == \"tanh\":\n",
    "            return 1 - np.power(self.output, 2)\n",
    "        else:\n",
    "            raise ValueError(f\"No derivative implemented for activation function type: {self.type}\")\n",
    "\n",
    "class Parameters:\n",
    "    def __init__(self, input_size, num_neurons):\n",
    "        self.weights = np.random.randn(input_size, num_neurons) * 0.1\n",
    "        self.bias = np.random.randn(1, num_neurons) * 0.1\n",
    "\n",
    "    def get_weights(self):\n",
    "        return self.weights\n",
    "\n",
    "    def get_bias(self):\n",
    "        return self.bias\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, input_size):\n",
    "        self.params = Parameters(input_size, 1)  # Each neuron has one output\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs, self.params.get_weights()) + self.params.get_bias()\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, d_output):\n",
    "        # Reshape d_output if necessary to ensure it's 2D (batch_size, 1)\n",
    "        if d_output.ndim == 1:\n",
    "            d_output = d_output[:, np.newaxis]\n",
    "\n",
    "        self.d_weights = np.dot(self.inputs.T, d_output)\n",
    "        self.d_bias = np.sum(d_output, axis=0, keepdims=True)\n",
    "        self.d_inputs = np.dot(d_output, self.params.get_weights().T)\n",
    "\n",
    "        return self.d_inputs\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, input_size, num_neurons, activation_type):\n",
    "        self.neurons = [Neuron(input_size) for _ in range(num_neurons)]\n",
    "        self.activation_fn = Activation(activation_type)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        neuron_outputs = np.hstack([neuron.forward(inputs) for neuron in self.neurons])\n",
    "        return self.activation_fn.forward(neuron_outputs)\n",
    "\n",
    "    def backward(self, d_output):\n",
    "        d_output_activation = self.activation_fn.derivative() * d_output\n",
    "        d_inputs = np.zeros_like(self.inputs, dtype=np.float64)\n",
    "        for i, neuron in enumerate(self.neurons):\n",
    "            neuron_d_inputs = neuron.backward(d_output_activation[:, i:i+1])\n",
    "            d_inputs += neuron_d_inputs\n",
    "        return d_inputs\n",
    "\n",
    "class LossFunction:\n",
    "    @staticmethod\n",
    "    def mse(predicted, actual):\n",
    "        return np.mean((predicted - actual) ** 2)\n",
    "\n",
    "    @staticmethod\n",
    "    def mse_derivative(predicted, actual):\n",
    "        return 2 * (predicted - actual) / actual.size\n",
    "    \n",
    "class Dropout:\n",
    "    def __init__(self, rate):\n",
    "        self.rate = rate\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, inputs, training=True):\n",
    "        if training:\n",
    "            self.mask = np.random.binomial(1, 1 - self.rate, size=inputs.shape) / (1 - self.rate)\n",
    "            self.output = inputs * self.mask\n",
    "        else:\n",
    "            self.output = inputs\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, d_output):\n",
    "        # Only propagate gradients where the mask is not zero\n",
    "        self.d_inputs = d_output * self.mask\n",
    "        return self.d_inputs\n",
    "\n",
    "class Regularization:\n",
    "    @staticmethod\n",
    "    def l2(weights, lambda_val):\n",
    "        return (lambda_val / 2) * np.sum(weights ** 2)\n",
    "\n",
    "    @staticmethod\n",
    "    def l2_derivative(weights, lambda_val):\n",
    "        return lambda_val * weights\n",
    "    \n",
    "class BackPropagation:\n",
    "    @staticmethod\n",
    "    def update_parameters(layers, learning_rate, lambda_val=0.01):\n",
    "        for layer in layers:\n",
    "            if hasattr(layer, 'neurons'):\n",
    "                for neuron in layer.neurons:\n",
    "                    neuron.params.weights -= learning_rate * (neuron.d_weights + Regularization.l2_derivative(neuron.params.weights, lambda_val))\n",
    "                    neuron.params.bias -= learning_rate * neuron.d_bias\n",
    "                    \n",
    "                    \n",
    "class InputNormalization:\n",
    "    def __init__(self, method='zscore'):\n",
    "        self.method = method\n",
    "        self.min = None\n",
    "        self.max = None\n",
    "        self.mean = None\n",
    "        self.std = None\n",
    "\n",
    "    def fit(self, data):\n",
    "        if self.method == 'minmax':\n",
    "            self.min = np.min(data, axis=0)\n",
    "            self.max = np.max(data, axis=0)\n",
    "        elif self.method == 'zscore':\n",
    "            self.mean = np.mean(data, axis=0)\n",
    "            self.std = np.std(data, axis=0)\n",
    "            self.std[self.std == 0] = 1  # Prevent division by zero for features with no variation\n",
    "        elif self.method == 'max':\n",
    "            self.max = np.max(np.abs(data), axis=0)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown normalization method: {self.method}\")\n",
    "\n",
    "    def transform(self, data):\n",
    "        if self.method == 'minmax':\n",
    "            return (data - self.min) / (self.max - self.min)\n",
    "        elif self.method == 'zscore':\n",
    "            return (data - self.mean) / self.std\n",
    "        elif self.method == 'max':\n",
    "            return data / self.max\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown normalization method: {self.method}\")\n",
    "\n",
    "    def fit_transform(self, data):\n",
    "        self.fit(data)\n",
    "        return self.transform(data)\n",
    "\n",
    "                    \n",
    "                    \n",
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "\n",
    "    def add_layer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        for layer in self.layers:\n",
    "            inputs = layer.forward(inputs)\n",
    "        return inputs\n",
    "\n",
    "    def backward(self, loss_gradient):\n",
    "        for layer in reversed(self.layers):\n",
    "            loss_gradient = layer.backward(loss_gradient)\n",
    "            \n",
    "    def train(self, inputs, targets, epochs, learning_rate, lambda_val=0.01):\n",
    "        for epoch in range(epochs):\n",
    "            outputs = self.forward(inputs)\n",
    "            loss = LossFunction.mse(outputs, targets)\n",
    "            # Add regularization loss\n",
    "            reg_loss = 0\n",
    "            for layer in self.layers:\n",
    "                if hasattr(layer, 'neurons'):\n",
    "                    for neuron in layer.neurons:\n",
    "                        reg_loss += Regularization.l2(neuron.params.weights, lambda_val)\n",
    "            loss += reg_loss\n",
    "            print(f\"Epoch {epoch+1}, Loss: {loss}\")\n",
    "            loss_gradient = LossFunction.mse_derivative(outputs, targets)\n",
    "            self.backward(loss_gradient)\n",
    "            BackPropagation.update_parameters(self.layers, learning_rate, lambda_val)\n",
    "\n",
    "            \n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        predictions = self.predict(X)\n",
    "        # Assuming binary classification or single-label multiclass classification\n",
    "        predictions = np.round(predictions)  # Adjust as needed for your case\n",
    "        accuracy = np.mean(predictions == y)\n",
    "        return accuracy\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        return self.forward(inputs)\n",
    "\n",
    "\n",
    "dataset = pd.read_csv(\"train_x.csv\")\n",
    "labels = pd.read_csv(\"train_label.csv\")\n",
    "\n",
    "# Analyzing the shapes of the dataset and labels\n",
    "dataset_shape = dataset.shape\n",
    "labels_shape = labels.shape\n",
    "\n",
    "dataset_shape, labels_shape\n",
    "\n",
    "input_normalizer = InputNormalization(method='zscore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the dataset into training, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(dataset, labels, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Checking the shapes of the splits\n",
    "(X_train.shape, y_train.shape), (X_val.shape, y_val.shape), (X_test.shape, y_test.shape)\n",
    "\n",
    "\n",
    "# Convert the pandas dataframes to numpy arrays for processing with the neural network\n",
    "X_train_np = input_normalizer.fit_transform(X_train.to_numpy())\n",
    "X_val_np = input_normalizer.transform(X_val.to_numpy())\n",
    "X_test_np = input_normalizer.transform(X_test.to_numpy())\n",
    "\n",
    "y_train_np = y_train.to_numpy()\n",
    "y_val_np = y_val.to_numpy()\n",
    "y_test_np = y_test.to_numpy()\n",
    "# Create the neural network\n",
    "nn = NeuralNetwork()\n",
    "\n",
    "input_size = 784  \n",
    "output_size = 10  \n",
    "\n",
    "nn.add_layer(Layer(input_size=input_size, num_neurons=64, activation_type='relu'))\n",
    "nn.add_layer(Dropout(rate=0.1))  # Dropout layer with a 50% dropout rate\n",
    "nn.add_layer(Layer(input_size=64, num_neurons=output_size, activation_type='sigmoid'))\n",
    "\n",
    "\n",
    "# Training parameters\n",
    "epochs = 100\n",
    "learning_rate = 0.1\n",
    "lambda_val = 0.001\n",
    "# Training the neural network\n",
    "print(\"Training the Neural Network...\")\n",
    "nn.train(X_train_np, y_train_np, epochs, learning_rate, lambda_val)\n",
    "\n",
    "# Evaluating the neural network\n",
    "print(\"Evaluating on Validation Data...\")\n",
    "val_accuracy = nn.evaluate(X_val_np, y_val_np)\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "print(\"Evaluating on Test Data...\")\n",
    "test_accuracy = nn.evaluate(X_test_np, y_test_np)\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db85c3cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
