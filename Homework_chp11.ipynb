{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55ba31ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the Neural Network...\n",
      "Epoch 1, Loss: 0.5346658424457715\n",
      "Epoch 2, Loss: 0.5218413679912367\n",
      "Epoch 3, Loss: 0.5149863017533929\n",
      "Epoch 4, Loss: 0.49994152985059037\n",
      "Epoch 5, Loss: 0.492146124534399\n",
      "Epoch 6, Loss: 0.4810275834759865\n",
      "Epoch 7, Loss: 0.4737761253797679\n",
      "Epoch 8, Loss: 0.4637310687990881\n",
      "Epoch 9, Loss: 0.45615837438450496\n",
      "Epoch 10, Loss: 0.44610000726225485\n",
      "Epoch 11, Loss: 0.44134480533294435\n",
      "Epoch 12, Loss: 0.433928756087926\n",
      "Epoch 13, Loss: 0.42894782591138847\n",
      "Epoch 14, Loss: 0.4235193691625566\n",
      "Epoch 15, Loss: 0.4179444387140303\n",
      "Epoch 16, Loss: 0.4120841017887417\n",
      "Epoch 17, Loss: 0.40828744644376086\n",
      "Epoch 18, Loss: 0.403898432019538\n",
      "Epoch 19, Loss: 0.40004941086103724\n",
      "Epoch 20, Loss: 0.3966141087832661\n",
      "Epoch 21, Loss: 0.39259032624749357\n",
      "Epoch 22, Loss: 0.39048845776111096\n",
      "Epoch 23, Loss: 0.38731016060959284\n",
      "Epoch 24, Loss: 0.38457647834334086\n",
      "Epoch 25, Loss: 0.3816470071576611\n",
      "Epoch 26, Loss: 0.37911236247300617\n",
      "Epoch 27, Loss: 0.3768753545490322\n",
      "Epoch 28, Loss: 0.3752117600648601\n",
      "Epoch 29, Loss: 0.3722426711885166\n",
      "Epoch 30, Loss: 0.37008888444145455\n",
      "Epoch 31, Loss: 0.3698366697594957\n",
      "Epoch 32, Loss: 0.3682091093166566\n",
      "Epoch 33, Loss: 0.36685794028355956\n",
      "Epoch 34, Loss: 0.364452851097801\n",
      "Epoch 35, Loss: 0.3634144357599093\n",
      "Epoch 36, Loss: 0.36230543037133306\n",
      "Epoch 37, Loss: 0.36190672359012427\n",
      "Epoch 38, Loss: 0.36072042141235755\n",
      "Epoch 39, Loss: 0.35844600433340545\n",
      "Epoch 40, Loss: 0.3563052245614509\n",
      "Epoch 41, Loss: 0.3574043215746989\n",
      "Epoch 42, Loss: 0.35655292502851715\n",
      "Epoch 43, Loss: 0.35497599022237597\n",
      "Epoch 44, Loss: 0.35386583520283044\n",
      "Epoch 45, Loss: 0.35296864232416225\n",
      "Epoch 46, Loss: 0.3517658723000763\n",
      "Epoch 47, Loss: 0.35124875552528684\n",
      "Epoch 48, Loss: 0.3504015600039436\n",
      "Epoch 49, Loss: 0.3508455242997885\n",
      "Epoch 50, Loss: 0.3497661922800124\n",
      "Epoch 51, Loss: 0.34818249340721513\n",
      "Epoch 52, Loss: 0.3477244274249679\n",
      "Epoch 53, Loss: 0.3469706492914322\n",
      "Epoch 54, Loss: 0.3473685417225486\n",
      "Epoch 55, Loss: 0.3456369499829679\n",
      "Epoch 56, Loss: 0.3453789448122465\n",
      "Epoch 57, Loss: 0.344440324067986\n",
      "Epoch 58, Loss: 0.3445797993279617\n",
      "Epoch 59, Loss: 0.34319417497652244\n",
      "Epoch 60, Loss: 0.34313849278581476\n",
      "Epoch 61, Loss: 0.3419421644002531\n",
      "Epoch 62, Loss: 0.341888331417333\n",
      "Epoch 63, Loss: 0.3415752154386429\n",
      "Epoch 64, Loss: 0.34129546197163896\n",
      "Epoch 65, Loss: 0.3403767191310488\n",
      "Epoch 66, Loss: 0.3401955518984451\n",
      "Epoch 67, Loss: 0.33953777299199306\n",
      "Epoch 68, Loss: 0.3394834483374755\n",
      "Epoch 69, Loss: 0.3390347643365676\n",
      "Epoch 70, Loss: 0.33991900080237547\n",
      "Epoch 71, Loss: 0.3379486143013503\n",
      "Epoch 72, Loss: 0.3382162746840439\n",
      "Epoch 73, Loss: 0.3378577983662115\n",
      "Epoch 74, Loss: 0.3373903843362033\n",
      "Epoch 75, Loss: 0.3368418596484254\n",
      "Epoch 76, Loss: 0.33565689362507517\n",
      "Epoch 77, Loss: 0.33639197335071763\n",
      "Epoch 78, Loss: 0.3360257485658217\n",
      "Epoch 79, Loss: 0.3352323768825526\n",
      "Epoch 80, Loss: 0.3345918005207414\n",
      "Epoch 81, Loss: 0.3357618562067406\n",
      "Epoch 82, Loss: 0.3341930536535552\n",
      "Epoch 83, Loss: 0.3346877852632839\n",
      "Epoch 84, Loss: 0.3343378669857959\n",
      "Epoch 85, Loss: 0.33397495002095046\n",
      "Epoch 86, Loss: 0.3332499286530575\n",
      "Epoch 87, Loss: 0.3334457760439875\n",
      "Epoch 88, Loss: 0.3331067665305048\n",
      "Epoch 89, Loss: 0.33290367260003084\n",
      "Epoch 90, Loss: 0.33217452635552014\n",
      "Epoch 91, Loss: 0.3320000678272517\n",
      "Epoch 92, Loss: 0.3321170510151553\n",
      "Epoch 93, Loss: 0.33076690756047417\n",
      "Epoch 94, Loss: 0.3304244928085064\n",
      "Epoch 95, Loss: 0.3306357579091328\n",
      "Epoch 96, Loss: 0.3306414088026315\n",
      "Epoch 97, Loss: 0.33085958512316477\n",
      "Epoch 98, Loss: 0.3301797717266793\n",
      "Epoch 99, Loss: 0.3301353526758437\n",
      "Epoch 100, Loss: 0.3299902938649991\n",
      "Evaluating on Validation Data...\n",
      "Validation Accuracy: 0.909\n",
      "Evaluating on Test Data...\n",
      "Test Accuracy: 0.905\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class Activation:\n",
    "    def __init__(self, type):\n",
    "        self.type = type\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        if self.type == \"relu\":\n",
    "            self.output = np.maximum(0, inputs)\n",
    "        elif self.type == \"sigmoid\":\n",
    "        # Clipping inputs to avoid overflow\n",
    "            clipped_inputs = np.clip(inputs, -709, 709)  # np.exp(709) is close to the limit\n",
    "            self.output = 1 / (1 + np.exp(-clipped_inputs))\n",
    "        elif self.type == \"linear\":\n",
    "            self.output = inputs\n",
    "        elif self.type == \"tanh\":\n",
    "            self.output = np.tanh(inputs)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid activation function type: {self.type}\")\n",
    "        return self.output\n",
    "\n",
    "    def derivative(self):\n",
    "        if self.type == \"relu\":\n",
    "            return np.where(self.inputs > 0, 1, 0)\n",
    "        elif self.type == \"sigmoid\":\n",
    "            return self.output * (1 - self.output)\n",
    "        elif self.type == \"linear\":\n",
    "            return np.ones_like(self.inputs)\n",
    "        elif self.type == \"tanh\":\n",
    "            return 1 - np.power(self.output, 2)\n",
    "        else:\n",
    "            raise ValueError(f\"No derivative implemented for activation function type: {self.type}\")\n",
    "\n",
    "class Parameters:\n",
    "    def __init__(self, input_size, num_neurons):\n",
    "        self.weights = np.random.randn(input_size, num_neurons) * 0.1\n",
    "        self.bias = np.random.randn(1, num_neurons) * 0.1\n",
    "\n",
    "    def get_weights(self):\n",
    "        return self.weights\n",
    "\n",
    "    def get_bias(self):\n",
    "        return self.bias\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, input_size):\n",
    "        self.params = Parameters(input_size, 1)  # Each neuron has one output\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs, self.params.get_weights()) + self.params.get_bias()\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, d_output):\n",
    "        # Reshape d_output if necessary to ensure it's 2D (batch_size, 1)\n",
    "        if d_output.ndim == 1:\n",
    "            d_output = d_output[:, np.newaxis]\n",
    "\n",
    "        self.d_weights = np.dot(self.inputs.T, d_output)\n",
    "        self.d_bias = np.sum(d_output, axis=0, keepdims=True)\n",
    "        self.d_inputs = np.dot(d_output, self.params.get_weights().T)\n",
    "\n",
    "        return self.d_inputs\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, input_size, num_neurons, activation_type):\n",
    "        self.neurons = [Neuron(input_size) for _ in range(num_neurons)]\n",
    "        self.activation_fn = Activation(activation_type)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        neuron_outputs = np.hstack([neuron.forward(inputs) for neuron in self.neurons])\n",
    "        return self.activation_fn.forward(neuron_outputs)\n",
    "\n",
    "    def backward(self, d_output):\n",
    "        d_output_activation = self.activation_fn.derivative() * d_output\n",
    "        d_inputs = np.zeros_like(self.inputs, dtype=np.float64)\n",
    "        for i, neuron in enumerate(self.neurons):\n",
    "            neuron_d_inputs = neuron.backward(d_output_activation[:, i:i+1])\n",
    "            d_inputs += neuron_d_inputs\n",
    "        return d_inputs\n",
    "\n",
    "class LossFunction:\n",
    "    @staticmethod\n",
    "    def mse(predicted, actual):\n",
    "        return np.mean((predicted - actual) ** 2)\n",
    "\n",
    "    @staticmethod\n",
    "    def mse_derivative(predicted, actual):\n",
    "        return 2 * (predicted - actual) / actual.size\n",
    "    \n",
    "class Dropout:\n",
    "    def __init__(self, rate):\n",
    "        self.rate = rate\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, inputs, training=True):\n",
    "        if training:\n",
    "            self.mask = np.random.binomial(1, 1 - self.rate, size=inputs.shape) / (1 - self.rate)\n",
    "            self.output = inputs * self.mask\n",
    "        else:\n",
    "            self.output = inputs\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, d_output):\n",
    "        # Only propagate gradients where the mask is not zero\n",
    "        self.d_inputs = d_output * self.mask\n",
    "        return self.d_inputs\n",
    "\n",
    "class Regularization:\n",
    "    @staticmethod\n",
    "    def l2(weights, lambda_val):\n",
    "        return (lambda_val / 2) * np.sum(weights ** 2)\n",
    "\n",
    "    @staticmethod\n",
    "    def l2_derivative(weights, lambda_val):\n",
    "        return lambda_val * weights\n",
    "    \n",
    "class BackPropagation:\n",
    "    @staticmethod\n",
    "    def update_parameters(layers, learning_rate, lambda_val=0.01):\n",
    "        for layer in layers:\n",
    "            if hasattr(layer, 'neurons'):\n",
    "                for neuron in layer.neurons:\n",
    "                    neuron.params.weights -= learning_rate * (neuron.d_weights + Regularization.l2_derivative(neuron.params.weights, lambda_val))\n",
    "                    neuron.params.bias -= learning_rate * neuron.d_bias\n",
    "                    \n",
    "                    \n",
    "class InputNormalization:\n",
    "    def __init__(self, method='zscore'):\n",
    "        self.method = method\n",
    "        self.min = None\n",
    "        self.max = None\n",
    "        self.mean = None\n",
    "        self.std = None\n",
    "\n",
    "    def fit(self, data):\n",
    "        if self.method == 'minmax':\n",
    "            self.min = np.min(data, axis=0)\n",
    "            self.max = np.max(data, axis=0)\n",
    "        elif self.method == 'zscore':\n",
    "            self.mean = np.mean(data, axis=0)\n",
    "            self.std = np.std(data, axis=0)\n",
    "            self.std[self.std == 0] = 1  # Prevent division by zero for features with no variation\n",
    "        elif self.method == 'max':\n",
    "            self.max = np.max(np.abs(data), axis=0)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown normalization method: {self.method}\")\n",
    "\n",
    "    def transform(self, data):\n",
    "        if self.method == 'minmax':\n",
    "            return (data - self.min) / (self.max - self.min)\n",
    "        elif self.method == 'zscore':\n",
    "            return (data - self.mean) / self.std\n",
    "        elif self.method == 'max':\n",
    "            return data / self.max\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown normalization method: {self.method}\")\n",
    "\n",
    "    def fit_transform(self, data):\n",
    "        self.fit(data)\n",
    "        return self.transform(data)\n",
    "\n",
    "    \n",
    "    \n",
    "class MiniBatchGenerator:\n",
    "    def __init__(self, X, y, batch_size=32):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.n_samples = X.shape[0]\n",
    "        self.indices = np.arange(self.n_samples)\n",
    "        np.random.shuffle(self.indices)\n",
    "\n",
    "    def get_batches(self):\n",
    "        for start_idx in range(0, self.n_samples, self.batch_size):\n",
    "            end_idx = min(start_idx + self.batch_size, self.n_samples)\n",
    "            batch_indices = self.indices[start_idx:end_idx]\n",
    "            yield self.X[batch_indices], self.y[batch_indices]\n",
    "                    \n",
    "                    \n",
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "\n",
    "    def add_layer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        for layer in self.layers:\n",
    "            inputs = layer.forward(inputs)\n",
    "        return inputs\n",
    "\n",
    "    def backward(self, loss_gradient):\n",
    "        for layer in reversed(self.layers):\n",
    "            loss_gradient = layer.backward(loss_gradient)\n",
    "            \n",
    "    def train(self, X, y, epochs, learning_rate, lambda_val, batch_size):\n",
    "        minibatch_generator = MiniBatchGenerator(X, y, batch_size)\n",
    "        for epoch in range(epochs):\n",
    "            for X_batch, y_batch in minibatch_generator.get_batches():\n",
    "                outputs = self.forward(inputs)\n",
    "                loss = LossFunction.mse(outputs, targets)\n",
    "                # Add regularization loss\n",
    "                reg_loss = 0\n",
    "                for layer in self.layers:\n",
    "                    if hasattr(layer, 'neurons'):\n",
    "                        for neuron in layer.neurons:\n",
    "                            reg_loss += Regularization.l2(neuron.params.weights, lambda_val)\n",
    "                loss += reg_loss\n",
    "                print(f\"Epoch {epoch+1}, Loss: {loss}\")\n",
    "                loss_gradient = LossFunction.mse_derivative(outputs, targets)\n",
    "                self.backward(loss_gradient)\n",
    "                BackPropagation.update_parameters(self.layers, learning_rate, lambda_val)\n",
    "            \n",
    "    def train(self, inputs, targets, epochs, learning_rate, lambda_val=0.01):\n",
    "        for epoch in range(epochs):\n",
    "            outputs = self.forward(inputs)\n",
    "            loss = LossFunction.mse(outputs, targets)\n",
    "            # Add regularization loss\n",
    "            reg_loss = 0\n",
    "            for layer in self.layers:\n",
    "                if hasattr(layer, 'neurons'):\n",
    "                    for neuron in layer.neurons:\n",
    "                        reg_loss += Regularization.l2(neuron.params.weights, lambda_val)\n",
    "            loss += reg_loss\n",
    "            print(f\"Epoch {epoch+1}, Loss: {loss}\")\n",
    "            loss_gradient = LossFunction.mse_derivative(outputs, targets)\n",
    "            self.backward(loss_gradient)\n",
    "            BackPropagation.update_parameters(self.layers, learning_rate, lambda_val)\n",
    "\n",
    "            \n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        predictions = self.predict(X)\n",
    "        # Assuming binary classification or single-label multiclass classification\n",
    "        predictions = np.round(predictions)  # Adjust as needed for your case\n",
    "        accuracy = np.mean(predictions == y)\n",
    "        return accuracy\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        return self.forward(inputs)\n",
    "\n",
    "\n",
    "dataset = pd.read_csv(\"train_x.csv\")\n",
    "labels = pd.read_csv(\"train_label.csv\")\n",
    "\n",
    "# Analyzing the shapes of the dataset and labels\n",
    "dataset_shape = dataset.shape\n",
    "labels_shape = labels.shape\n",
    "\n",
    "dataset_shape, labels_shape\n",
    "\n",
    "input_normalizer = InputNormalization(method='zscore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the dataset into training, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(dataset, labels, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Checking the shapes of the splits\n",
    "(X_train.shape, y_train.shape), (X_val.shape, y_val.shape), (X_test.shape, y_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "# Convert the pandas dataframes to numpy arrays for processing with the neural network\n",
    "X_train_np = input_normalizer.fit_transform(X_train.to_numpy())\n",
    "X_val_np = input_normalizer.transform(X_val.to_numpy())\n",
    "X_test_np = input_normalizer.transform(X_test.to_numpy())\n",
    "\n",
    "y_train_np = y_train.to_numpy()\n",
    "y_val_np = y_val.to_numpy()\n",
    "y_test_np = y_test.to_numpy()\n",
    "# Create the neural network\n",
    "nn = NeuralNetwork()\n",
    "\n",
    "input_size = 784  \n",
    "output_size = 10  \n",
    "\n",
    "nn.add_layer(Layer(input_size=input_size, num_neurons=64, activation_type='relu'))\n",
    "nn.add_layer(Dropout(rate=0.1))  # Dropout layer with a 50% dropout rate\n",
    "nn.add_layer(Layer(input_size=64, num_neurons=output_size, activation_type='sigmoid'))\n",
    "\n",
    "\n",
    "# Training parameters\n",
    "epochs = 100\n",
    "learning_rate = 0.1\n",
    "lambda_val = 0.001\n",
    "# Training the neural network\n",
    "print(\"Training the Neural Network...\")\n",
    "nn.train(X_train_np, y_train_np, epochs, learning_rate, lambda_val)\n",
    "\n",
    "# Evaluating the neural network\n",
    "print(\"Evaluating on Validation Data...\")\n",
    "val_accuracy = nn.evaluate(X_val_np, y_val_np)\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "print(\"Evaluating on Test Data...\")\n",
    "test_accuracy = nn.evaluate(X_test_np, y_test_np)\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074060d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
