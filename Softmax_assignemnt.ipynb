{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6543b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.1083590775700618\n",
      "Epoch 100, Loss: 1.099348049457084\n",
      "Epoch 200, Loss: 1.0935887165232463\n",
      "Epoch 300, Loss: 1.089647785169491\n",
      "Epoch 400, Loss: 1.0868079748497563\n",
      "Epoch 500, Loss: 1.0845982628097817\n",
      "Epoch 600, Loss: 1.0827435156664944\n",
      "Epoch 700, Loss: 1.0810951585037896\n",
      "Epoch 800, Loss: 1.0795647912896933\n",
      "Epoch 900, Loss: 1.0781048753364784\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Activation Class\n",
    "class Activation:\n",
    "    @staticmethod\n",
    "    def apply(aggregate_signal, activation_function='relu'):\n",
    "        if activation_function == 'relu':\n",
    "            return np.maximum(0, aggregate_signal)\n",
    "        elif activation_function == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-aggregate_signal))\n",
    "        elif activation_function == 'tanh':\n",
    "            return np.tanh(aggregate_signal)\n",
    "        elif activation_function == 'softmax':\n",
    "            e_x = np.exp(aggregate_signal - np.max(aggregate_signal, axis=1, keepdims=True))\n",
    "            return e_x / np.sum(e_x, axis=1, keepdims=True)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation function.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def derivative(aggregate_signal, activation_function='relu'):\n",
    "        if activation_function == 'relu':\n",
    "            return np.where(aggregate_signal > 0, 1, 0)\n",
    "        elif activation_function == 'sigmoid':\n",
    "            s = 1 / (1 + np.exp(-aggregate_signal))\n",
    "            return s * (1 - s)\n",
    "        elif activation_function == 'tanh':\n",
    "            return 1 - np.tanh(aggregate_signal) ** 2\n",
    "        # Note: Softmax derivative needs special handling\n",
    "\n",
    "# Neuron Class\n",
    "class Neuron:\n",
    "    def __init__(self, weights, bias):\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "\n",
    "    def process(self, input_data):\n",
    "        # Vectorized operation for efficiency\n",
    "        return np.dot(input_data, self.weights) + self.bias\n",
    "\n",
    "# Parameters Class\n",
    "class Parameters:\n",
    "    def __init__(self, layer_size, next_layer_size, learning_rate=0.001):\n",
    "        self.weights = np.random.randn(layer_size, next_layer_size) * 0.1\n",
    "        self.bias = np.zeros((1, next_layer_size))\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "# Layer Class\n",
    "class Layer:\n",
    "    def __init__(self, size, next_layer_size, activation_function):\n",
    "        self.neurons = [Neuron(np.random.randn(next_layer_size) * 0.1, 0) for _ in range(size)]\n",
    "        self.params = Parameters(size, next_layer_size)  # Parameters for vectorized operations\n",
    "        self.activation_function = activation_function\n",
    "        self.input_data = None\n",
    "        self.aggregate_signal = None \n",
    "\n",
    "# ForwardPropagation Class\n",
    "class ForwardPropagation:\n",
    "    @staticmethod\n",
    "    def apply(layer, input_data):\n",
    "        layer.input_data = input_data  # Store input data\n",
    "        aggregate_signal = np.dot(input_data, layer.params.weights) + layer.params.bias\n",
    "        layer.aggregate_signal = aggregate_signal  # Store aggregate signal\n",
    "        output = Activation.apply(aggregate_signal, layer.activation_function)\n",
    "        return output\n",
    "\n",
    "\n",
    "#LossFunction Class\n",
    "class LossFunction:\n",
    "    @staticmethod\n",
    "    def compute(output, target, function='cross_entropy'):\n",
    "        m = target.shape[0]\n",
    "        if function == 'mse':\n",
    "            return np.mean(np.power(target - output, 2))\n",
    "        elif function == 'cross_entropy':\n",
    "            return -np.sum(target * np.log(np.clip(output, 1e-10, 1 - 1e-10))) / m\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported loss function.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def derivative(output, target, function='cross_entropy'):\n",
    "        if function == 'mse':\n",
    "            return output - target\n",
    "        elif function == 'cross_entropy':\n",
    "            output_clipped = np.clip(output, 1e-10, 1 - 1e-10)\n",
    "            return - (target / output_clipped) + ((1 - target) / (1 - output_clipped))\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported loss function derivative.\")\n",
    "\n",
    "#GradientDescent Class            \n",
    "class GradientDescent:\n",
    "    @staticmethod\n",
    "    def update_params(layer, dW, dB):\n",
    "        layer.params.weights -= layer.params.learning_rate * dW\n",
    "        layer.params.bias -= layer.params.learning_rate * dB\n",
    "\n",
    "#BackwardPropagation\n",
    "class BackwardPropagation:\n",
    "    @staticmethod\n",
    "    def apply(network, output, target):\n",
    "        layers = network.layers[::-1]\n",
    "        for i, layer in enumerate(layers):\n",
    "            if i == 0:  # Output layer\n",
    "                if layer.activation_function == 'softmax':\n",
    "                    dZ = output - target\n",
    "                else:\n",
    "                    dZ = LossFunction.derivative(output, target, 'cross_entropy')\n",
    "            else:\n",
    "                prev_layer = layers[i-1]\n",
    "                dZ = np.dot(dZ, prev_layer.params.weights.T) * Activation.derivative(layer.aggregate_signal, layer.activation_function)\n",
    "\n",
    "            dW = np.dot(layer.input_data.T, dZ) / layer.input_data.shape[0]\n",
    "            dB = np.sum(dZ, axis=0, keepdims=True) / layer.input_data.shape[0]\n",
    "            GradientDescent.update_params(layer, dW, dB)\n",
    "\n",
    "#Model Class\n",
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "\n",
    "    def add_layer(self, size, next_layer_size, activation_function='relu'):\n",
    "        self.layers.append(Layer(size, next_layer_size, activation_function))\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        for layer in self.layers:\n",
    "            inputs = ForwardPropagation.apply(layer, inputs)\n",
    "        return inputs\n",
    "\n",
    "    def train(self, inputs, targets, epochs=1000):\n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            output = self.predict(inputs)\n",
    "            loss = LossFunction.compute(output, targets, 'cross_entropy')\n",
    "\n",
    "            # Backward pass\n",
    "            BackwardPropagation.apply(self, output, targets)\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "input_size = 100  # Number of input features\n",
    "hidden_layer_size = 50 \n",
    "hidden_layer_size2 = 70\n",
    "output_size = 3  # Number of output classes for a classification problem\n",
    "learning_rate = 0.001  # Learning rate for gradient descent\n",
    "\n",
    "model = Model()\n",
    "model.add_layer(input_size, hidden_layer_size, 'relu')\n",
    "model.add_layer(hidden_layer_size, hidden_layer_size2, 'tanh')\n",
    "model.add_layer(hidden_layer_size2, hidden_layer_size2, 'tanh')\n",
    "model.add_layer(hidden_layer_size2, output_size, 'softmax')\n",
    "\n",
    "np.random.seed(0)\n",
    "inputs = np.random.rand(100, input_size)  # 100 samples\n",
    "targets = np.zeros((100, output_size))\n",
    "for i in range(100):\n",
    "    targets[i, np.random.randint(0, output_size)] = 1  # Random targets for demonstration\n",
    "\n",
    "# Train the model\n",
    "model.train(inputs, targets, epochs=1000)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ec2f86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
