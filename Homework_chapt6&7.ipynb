{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c49eb5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6a220b",
   "metadata": {},
   "source": [
    "# Activation Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2124ce3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation:\n",
    "    def __init__(self, type):\n",
    "        self.type = type\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        if self.type == \"relu\":\n",
    "            self.output = np.maximum(0, inputs)\n",
    "        elif self.type == \"sigmoid\":\n",
    "            self.output = 1 / (1 + np.exp(-inputs))\n",
    "        elif self.type == \"linear\":\n",
    "            self.output = inputs\n",
    "        elif self.type == \"tanh\":\n",
    "            self.output = np.tanh(inputs)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid activation function type: {self.type}\")\n",
    "        return self.output\n",
    "\n",
    "    def derivative(self):\n",
    "        if self.type == \"relu\":\n",
    "            return np.where(self.inputs > 0, 1, 0)\n",
    "        elif self.type == \"sigmoid\":\n",
    "            return self.output * (1 - self.output)\n",
    "        elif self.type == \"linear\":\n",
    "            return np.ones_like(self.inputs)\n",
    "        elif self.type == \"tanh\":\n",
    "            return 1 - np.power(self.output, 2)\n",
    "        else:\n",
    "            raise ValueError(f\"No derivative implemented for activation function type: {self.type}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f535d83",
   "metadata": {},
   "source": [
    "# Parameters Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "daa93532",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameters:\n",
    "    def __init__(self, input_size, num_neurons):\n",
    "        self.weights = np.random.randn(input_size, num_neurons) * 0.1\n",
    "        self.bias = np.random.randn(1, num_neurons) * 0.1\n",
    "\n",
    "    def get_weights(self):\n",
    "        return self.weights\n",
    "\n",
    "    def get_bias(self):\n",
    "        return self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3598810",
   "metadata": {},
   "source": [
    "# Neuron Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c4fcf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, input_size):\n",
    "        self.params = Parameters(input_size, 1)  # Each neuron has one output\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs, self.params.get_weights()) + self.params.get_bias()\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, d_output):\n",
    "        # Reshape d_output if necessary to ensure it's 2D (batch_size, 1)\n",
    "        if d_output.ndim == 1:\n",
    "            d_output = d_output[:, np.newaxis]\n",
    "\n",
    "        # Now d_output is properly shaped for the dot product\n",
    "        self.d_weights = np.dot(self.inputs.T, d_output)\n",
    "        self.d_bias = np.sum(d_output, axis=0, keepdims=True)\n",
    "        self.d_inputs = np.dot(d_output, self.params.get_weights().T)\n",
    "\n",
    "        # If d_inputs is 2D with a singleton second dimension, it can be reshaped for the next layer\n",
    "        if self.d_inputs.shape[-1] == 1:\n",
    "            self.d_inputs = self.d_inputs.reshape(self.inputs.shape)\n",
    "\n",
    "        return self.d_inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e229783",
   "metadata": {},
   "source": [
    "# Layer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5134abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, input_size, num_neurons, activation_type):\n",
    "        self.neurons = [Neuron(input_size) for _ in range(num_neurons)]\n",
    "        self.activation_fn = Activation(activation_type)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        neuron_outputs = np.hstack([neuron.forward(inputs) for neuron in self.neurons])\n",
    "        return self.activation_fn.forward(neuron_outputs)\n",
    "\n",
    "    def backward(self, d_output):\n",
    "        d_output_activation = self.activation_fn.derivative() * d_output\n",
    "        # Ensure d_inputs is initialized as a float array to match gradient data types\n",
    "        d_inputs = np.zeros_like(self.inputs, dtype=np.float64)\n",
    "        for i, neuron in enumerate(self.neurons):\n",
    "            neuron_d_inputs = neuron.backward(d_output_activation[:, i:i+1])\n",
    "            d_inputs += neuron_d_inputs\n",
    "        return d_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db805e69",
   "metadata": {},
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6813511b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackPropagation:\n",
    "    @staticmethod\n",
    "    def update_parameters(layers, learning_rate):\n",
    "        for layer in layers:\n",
    "            for neuron in layer.neurons:\n",
    "                neuron.params.weights -= learning_rate * neuron.d_weights\n",
    "                neuron.params.bias -= learning_rate * neuron.d_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ec9f56",
   "metadata": {},
   "source": [
    "# NeuralNetwork Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3752bacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "\n",
    "    def add_layer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        for layer in self.layers:\n",
    "            inputs = layer.forward(inputs)\n",
    "        return inputs\n",
    "\n",
    "    def backward(self, loss_gradient):\n",
    "        for layer in reversed(self.layers):\n",
    "            loss_gradient = layer.backward(loss_gradient)\n",
    "\n",
    "    def train(self, inputs, targets, epochs, learning_rate):\n",
    "        for epoch in range(epochs):\n",
    "            outputs = self.forward(inputs)\n",
    "            loss = LossFunction.mse(outputs, targets)\n",
    "            print(f\"Epoch {epoch+1}, Loss: {loss}\")\n",
    "            loss_gradient = LossFunction.mse_derivative(outputs, targets)\n",
    "            self.backward(loss_gradient)\n",
    "            BackPropagation.update_parameters(self.layers, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563f7365",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab02f0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossFunction:\n",
    "    @staticmethod\n",
    "    def mse(predicted, actual):\n",
    "        return np.mean((predicted - actual) ** 2)\n",
    "\n",
    "    @staticmethod\n",
    "    def mse_derivative(predicted, actual):\n",
    "        return 2 * (predicted - actual) / actual.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27543d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.5261628845479799\n",
      "Epoch 2, Loss: 0.515145555931176\n",
      "Epoch 3, Loss: 0.5045588129608033\n",
      "Epoch 4, Loss: 0.49439068723076507\n",
      "Epoch 5, Loss: 0.4846288642084503\n",
      "Epoch 6, Loss: 0.4752607819185806\n",
      "Epoch 7, Loss: 0.4662737207029771\n",
      "Epoch 8, Loss: 0.4576548841302702\n",
      "Epoch 9, Loss: 0.44939147126342605\n",
      "Epoch 10, Loss: 0.44147074059992303\n",
      "Epoch 11, Loss: 0.43388006608170404\n",
      "Epoch 12, Loss: 0.4266069856321897\n",
      "Epoch 13, Loss: 0.4196392427183613\n",
      "Epoch 14, Loss: 0.4129648214599608\n",
      "Epoch 15, Loss: 0.40657197581783194\n",
      "Epoch 16, Loss: 0.4004492533918674\n",
      "Epoch 17, Loss: 0.3945855143482019\n",
      "Epoch 18, Loss: 0.38896994597727946\n",
      "Epoch 19, Loss: 0.38359207336101914\n",
      "Epoch 20, Loss: 0.378441766600089\n",
      "Epoch 21, Loss: 0.37350924502257665\n",
      "Epoch 22, Loss: 0.3687850787642409\n",
      "Epoch 23, Loss: 0.3642601880789152\n",
      "Epoch 24, Loss: 0.3599258407062458\n",
      "Epoch 25, Loss: 0.3557736475933194\n",
      "Epoch 26, Loss: 0.3517955572373006\n",
      "Epoch 27, Loss: 0.3479838488882407\n",
      "Epoch 28, Loss: 0.3443311248249553\n",
      "Epoch 29, Loss: 0.34083030189240204\n",
      "Epoch 30, Loss: 0.3374746024663932\n",
      "Epoch 31, Loss: 0.3342575449907537\n",
      "Epoch 32, Loss: 0.3311729342131456\n",
      "Epoch 33, Loss: 0.3282148512286697\n",
      "Epoch 34, Loss: 0.32537764342494224\n",
      "Epoch 35, Loss: 0.32265591440852526\n",
      "Epoch 36, Loss: 0.32004451398026723\n",
      "Epoch 37, Loss: 0.317538528216168\n",
      "Epoch 38, Loss: 0.3151332697007134\n",
      "Epoch 39, Loss: 0.31282426795111695\n",
      "Epoch 40, Loss: 0.31060726006345507\n",
      "Epoch 41, Loss: 0.30847818160518364\n",
      "Epoch 42, Loss: 0.30643315777288666\n",
      "Epoch 43, Loss: 0.3044684948292342\n",
      "Epoch 44, Loss: 0.30258067182894743\n",
      "Epoch 45, Loss: 0.30076633263998453\n",
      "Epoch 46, Loss: 0.2990222782631347\n",
      "Epoch 47, Loss: 0.29734545945063573\n",
      "Epoch 48, Loss: 0.29573296962229423\n",
      "Epoch 49, Loss: 0.2941820380758062\n",
      "Epoch 50, Loss: 0.29269002348651413\n",
      "Epoch 51, Loss: 0.29125440769066085\n",
      "Epoch 52, Loss: 0.28987278974525155\n",
      "Epoch 53, Loss: 0.2885428802569057\n",
      "Epoch 54, Loss: 0.2872624959715225\n",
      "Epoch 55, Loss: 0.2860295546161772\n",
      "Epoch 56, Loss: 0.2848420699843953\n",
      "Epoch 57, Loss: 0.28369814725578274\n",
      "Epoch 58, Loss: 0.28259597854091767\n",
      "Epoch 59, Loss: 0.28153383864241865\n",
      "Epoch 60, Loss: 0.2805100810231665\n",
      "Epoch 61, Loss: 0.2795231339727826\n",
      "Epoch 62, Loss: 0.2785714969636279\n",
      "Epoch 63, Loss: 0.2776537371877867\n",
      "Epoch 64, Loss: 0.27676848626672174\n",
      "Epoch 65, Loss: 0.27591443712553426\n",
      "Epoch 66, Loss: 0.2750903410240219\n",
      "Epoch 67, Loss: 0.27429500473699964\n",
      "Epoch 68, Loss: 0.2735272878766237\n",
      "Epoch 69, Loss: 0.2727861003497444\n",
      "Epoch 70, Loss: 0.27207039994359145\n",
      "Epoch 71, Loss: 0.2713791900333789\n",
      "Epoch 72, Loss: 0.27071151740569416\n",
      "Epoch 73, Loss: 0.2700664701918066\n",
      "Epoch 74, Loss: 0.26944317590530187\n",
      "Epoch 75, Loss: 0.2688407995787051\n",
      "Epoch 76, Loss: 0.26825854199401056\n",
      "Epoch 77, Loss: 0.267695638002279\n",
      "Epoch 78, Loss: 0.26715135492770326\n",
      "Epoch 79, Loss: 0.2666249910517648\n",
      "Epoch 80, Loss: 0.2661158741733289\n",
      "Epoch 81, Loss: 0.2656233602407308\n",
      "Epoch 82, Loss: 0.2651468320521094\n",
      "Epoch 83, Loss: 0.2646856980204365\n",
      "Epoch 84, Loss: 0.264239390999872\n",
      "Epoch 85, Loss: 0.26380736717025255\n",
      "Epoch 86, Loss: 0.2633891049766869\n",
      "Epoch 87, Loss: 0.2629841041213896\n",
      "Epoch 88, Loss: 0.262591884605038\n",
      "Epoch 89, Loss: 0.2622119858150776\n",
      "Epoch 90, Loss: 0.2618439656585413\n",
      "Epoch 91, Loss: 0.26148739973707447\n",
      "Epoch 92, Loss: 0.2611418805619821\n",
      "Epoch 93, Loss: 0.2608070168072304\n",
      "Epoch 94, Loss: 0.2604824325984452\n",
      "Epoch 95, Loss: 0.2601677668360558\n",
      "Epoch 96, Loss: 0.2598626725508288\n",
      "Epoch 97, Loss: 0.2595668162901339\n",
      "Epoch 98, Loss: 0.25927987753337006\n",
      "Epoch 99, Loss: 0.2590015481350637\n",
      "Epoch 100, Loss: 0.258731531794234\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork()\n",
    "nn.add_layer(Layer(2, 3, \"relu\"))\n",
    "nn.add_layer(Layer(3, 1, \"tanh\"))\n",
    "\n",
    "inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "targets = np.array([[0], [1], [1], [0]])  \n",
    "\n",
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "nn.train(inputs, targets, epochs, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9900779",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
